version: '3'

x-spark-common: &spark-common
  image: bitnami/spark:latest
  build:
    context: .
    dockerfile: Dockerfile.spark  # Specify a custom Dockerfile for Spark
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs
    - ./misc:/opt/airflow/misc
    - ./data:/opt/airflow/data
  networks:
    - flowing-spark

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile.airflow # Specify a custom Dockerfile for Airflow
  env_file:
    - airflow.env
  volumes:
    - ./jobs:/opt/airflow/jobs
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./data:/opt/airflow/data
    - ./misc:/opt/airflow/misc
  depends_on:
    - postgres
  networks:
    - flowing-spark

services:
  spark-master:
    <<: *spark-common
    command: |
      /bin/bash -c "
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master
      "
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker-1:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 4g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-worker-2:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 4g
      SPARK_MASTER_URL: spark://spark-master:7077

  postgres:
    image: postgres:14.0
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    networks:
      - flowing-spark
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow" ]
      interval: 10s
      timeout: 5s
      retries: 5

 # Database initializer
  init:
    <<: *airflow-common
    command: bash -c "airflow db migrate && \
                      python /opt/airflow/dags/scripts/create_spark_connection.py"
                    # This is script has the consequence of the spark-submit operation
                    #   not working due to wrong "master" setup
                    #   yarn is used instead of the local spark-master-1

    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"  # Run only once, no restart needed

  scheduler:
    <<: *airflow-common
    command: bash -c "bash /opt/airflow/dags/scripts/activate_dag.sh && \
                    airflow db migrate && \
                    airflow users create --username admin --firstname Lukas --lastname Muszong --role Admin --email lukas.muszong@iubh.de --password admin && \
                    airflow scheduler"
    depends_on:
      postgres:
        condition: service_healthy
      init:
        condition: service_completed_successfully
    networks:
      - flowing-spark

  webserver:
    <<: *airflow-common
    command: bash -c "bash /opt/airflow/scripts/activate_dag.sh && airflow webserver"
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
      scheduler:
        condition: service_started
    networks:
      - flowing-spark

  grafana:
      image: grafana/grafana:latest
      container_name: grafana
      ports:
        - "3000:3000"  # Expose Grafana's web UI on port 3000
      environment:
        - GF_SECURITY_ADMIN_USER=admin  # Default admin username
        - GF_SECURITY_ADMIN_PASSWORD=admin  # Default admin password
      volumes:
        - grafana_data:/var/lib/grafana  # Persistent storage for Grafana data
        - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
        - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      depends_on:
        - postgres  # PostgreSQL as Grafana's data source
      networks:
        - flowing-spark  # Ensure Grafana is part of the same network as its data sources
      healthcheck:
        test: [ "CMD", "curl", "-f", "http://localhost:3000/health" ]
        interval: 30s
        timeout: 5s
        retries: 3

networks:
  flowing-spark:

volumes:
  pg_data:
  grafana_data: